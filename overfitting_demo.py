"""
Overfitting Analysis Demo for AIcrete UHPC Project
Copyright 2025 Shiksha Seechurn / AIcrete

This script demonstrates how to perform overfitting analysis on your UHPC models
to address supervisor feedback about missing overfitting considerations.
"""

import sys
import os
from Training import UHPCPredictor

def run_overfitting_analysis():
    """
    Run comprehensive overfitting analysis on UHPC models
    """
    
    print("ğŸ¯ AIcrete UHPC Overfitting Analysis Demo")
    print("=" * 50)
    print("Addressing supervisor feedback: 'Missing overfitting analysis'")
    print()
    
    # Initialize the UHPC predictor
    try:
        # Use your dataset file - adjust path as needed
        data_file = "AI Model Datasets Final with units.xlsx"
        
        if not os.path.exists(data_file):
            print(f"âŒ Dataset file not found: {data_file}")
            print("Please ensure the dataset file is in the current directory")
            return
        
        print(f"ğŸ“Š Loading dataset: {data_file}")
        predictor = UHPCPredictor(data_file)
        
        # Load and prepare data
        print("ğŸ”„ Preparing data...")
        predictor.load_and_prepare_data()
        
        # Train basic models first
        print("ğŸ¤– Training models...")
        predictor.train_models()
        
        # Now perform overfitting analysis
        print("\nğŸ” Starting comprehensive overfitting analysis...")
        predictor.analyze_overfitting()
        
        print("\nâœ… Overfitting analysis completed successfully!")
        print("\nğŸ“‹ What this analysis provides:")
        print("   âœ“ Training vs Test performance comparison")
        print("   âœ“ Learning curves to visualize overfitting")
        print("   âœ“ Cross-validation for robust evaluation")
        print("   âœ“ Overfitting risk assessment")
        print("   âœ“ Model-specific recommendations")
        
        print("\nğŸ“ Generated files:")
        print("   â€¢ overfitting_analysis_*.png - Comparison plots")
        print("   â€¢ learning_curves_*.png - Learning curve plots")
        print("   â€¢ cv_analysis_*.png - Cross-validation plots")
        
        print("\nğŸ“ For your research/dissertation:")
        print("   â€¢ Include these plots in your methodology section")
        print("   â€¢ Discuss overfitting risks and mitigation strategies")
        print("   â€¢ Compare model generalization across different properties")
        print("   â€¢ Reference cross-validation results for model reliability")
        
    except Exception as e:
        print(f"âŒ Error during analysis: {e}")
        print("Please check your dataset and try again")


def explain_overfitting():
    """
    Explain what overfitting means and why it's important
    """
    
    print("\nğŸ“š OVERFITTING EXPLAINED - What Your Supervisor Wants")
    print("=" * 60)
    
    print("""
ğŸ¯ What is Overfitting?
   Overfitting occurs when a model learns the training data too well,
   memorizing noise and specific patterns rather than general relationships.
   
ğŸš¨ Signs of Overfitting:
   â€¢ High training accuracy but poor test accuracy
   â€¢ Large gap between training and validation performance
   â€¢ Model performs well on your dataset but fails on new data
   
ğŸ” Why It Matters for UHPC Research:
   â€¢ Your model must work on NEW concrete mixes, not just your dataset
   â€¢ Real-world applicability depends on generalization
   â€¢ Industry adoption requires reliable predictions on unseen data
   
âœ… How This Analysis Helps:
   â€¢ Compares training vs test performance (gap analysis)
   â€¢ Shows learning curves to visualize overfitting behavior
   â€¢ Provides cross-validation for robust evaluation
   â€¢ Gives specific recommendations for each model
   
ğŸ“Š What to Include in Your Research:
   â€¢ Learning curves showing model behavior as data increases
   â€¢ Cross-validation scores demonstrating model reliability
   â€¢ Discussion of overfitting risks for each property prediction
   â€¢ Comparison of model generalization across different UHPC properties
   â€¢ Mitigation strategies (regularization, feature selection, etc.)
   
ğŸ“ Academic Impact:
   â€¢ Demonstrates understanding of ML best practices
   â€¢ Shows critical evaluation of model performance
   â€¢ Addresses generalization concerns proactively
   â€¢ Provides evidence of rigorous model validation
""")


if __name__ == "__main__":
    # Explain overfitting concept
    explain_overfitting()
    
    # Ask user if they want to run the analysis
    print("\n" + "="*60)
    response = input("Would you like to run the overfitting analysis now? (y/n): ").lower().strip()
    
    if response in ['y', 'yes']:
        run_overfitting_analysis()
    else:
        print("\nğŸ“ To run the analysis later, execute:")
        print("   python overfitting_demo.py")
        print("\nğŸ“š Or integrate into your main training script by calling:")
        print("   predictor.analyze_overfitting()")
